{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "Ans- To solve this problem, we can use Bayes' Theorem, which relates conditional probabilities. We want to find the probability that an employee is a smoker given that they use the health insurance plan, denoted as \\( P(\\text{Smoker}|\\text{Uses Insurance}) \\). \n",
    "\n",
    "According to Bayes' Theorem:\n",
    "\n",
    "\\[ P(\\text{Smoker}|\\text{Uses Insurance}) = \\frac{P(\\text{Uses Insurance}|\\text{Smoker}) \\times P(\\text{Smoker})}{P(\\text{Uses Insurance})} \\]\n",
    "\n",
    "Given:\n",
    "- \\( P(\\text{Uses Insurance}) = 0.70 \\) (probability of an employee using the health insurance plan)\n",
    "- \\( P(\\text{Smoker}|\\text{Uses Insurance}) = 0.40 \\) (probability of a smoker among those who use the health insurance plan)\n",
    "\n",
    "We need to calculate \\( P(\\text{Uses Insurance}|\\text{Smoker}) \\times P(\\text{Smoker}) \\).\n",
    "\n",
    "\\( P(\\text{Uses Insurance}|\\text{Smoker}) \\) represents the probability that an employee uses the health insurance plan given that they are a smoker. This information is not provided directly, but we can deduce it from the given data:\n",
    "\n",
    "\\[ P(\\text{Uses Insurance}|\\text{Smoker}) = 0.40 \\]\n",
    "\n",
    "\\( P(\\text{Smoker}) \\) represents the overall probability that an employee is a smoker. This information is not directly provided either, but we can deduce it from the given data:\n",
    "\n",
    "\\[ P(\\text{Smoker}) = ? \\]\n",
    "\n",
    "To find \\( P(\\text{Smoker}) \\), we can use the Law of Total Probability:\n",
    "\n",
    "\\[ P(\\text{Smoker}) = P(\\text{Smoker} \\cap \\text{Uses Insurance}) + P(\\text{Smoker} \\cap \\text{Does Not Use Insurance}) \\]\n",
    "\n",
    "Given:\n",
    "- \\( P(\\text{Smoker} \\cap \\text{Uses Insurance}) = P(\\text{Uses Insurance}|\\text{Smoker}) \\times P(\\text{Smoker}) = 0.40 \\times P(\\text{Smoker}) \\)\n",
    "- \\( P(\\text{Does Not Use Insurance}) = 1 - P(\\text{Uses Insurance}) = 1 - 0.70 = 0.30 \\)\n",
    "\n",
    "We have:\n",
    "\\[ P(\\text{Smoker}) = 0.40 \\times P(\\text{Smoker}) + 0.30 \\]\n",
    "\n",
    "Solving for \\( P(\\text{Smoker}) \\):\n",
    "\\[ P(\\text{Smoker}) = \\frac{0.30}{0.60} = 0.50 \\]\n",
    "\n",
    "Now, we can plug all the values into Bayes' Theorem to find \\( P(\\text{Smoker}|\\text{Uses Insurance}) \\):\n",
    "\n",
    "\\[ P(\\text{Smoker}|\\text{Uses Insurance}) = \\frac{0.40 \\times 0.50}{0.70} = \\frac{0.20}{0.70} \\approx 0.286 \\]\n",
    "\n",
    "So, the probability that an employee is a smoker given that they use the health insurance plan is approximately 0.286 or 28.6%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "Ans- \n",
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are both variants of the Naive Bayes classifier, but they differ in terms of the types of features they are designed to handle and the underlying probability distributions they assume. Here are the key differences between the two:\n",
    "\n",
    "Feature Representation:\n",
    "\n",
    "Bernoulli Naive Bayes: This classifier is designed for binary feature vectors, where each feature represents the presence or absence of a particular term or attribute. Typically used in text classification tasks where the presence or absence of specific words or terms in a document is relevant.\n",
    "Multinomial Naive Bayes: This classifier is suitable for feature vectors representing counts or frequencies of occurrences. It's commonly used in text classification tasks where features represent the frequency of words or terms in a document.\n",
    "Underlying Probability Distribution:\n",
    "\n",
    "Bernoulli Naive Bayes: Assumes a Bernoulli distribution for each feature, which means that each feature is assumed to be binary (0 or 1).\n",
    "Multinomial Naive Bayes: Assumes a multinomial distribution for each feature, which allows for non-binary integer counts or frequencies.\n",
    "Usage:\n",
    "\n",
    "Bernoulli Naive Bayes: Often used in tasks where the presence or absence of certain features is indicative of the class label. For example, document classification tasks where the presence of specific words in a document may indicate its category.\n",
    "Multinomial Naive Bayes: Commonly used in tasks where the frequency or count of features is important for classification. For example, document classification tasks where the frequency of words in a document is relevant for determining its category.\n",
    "Scalability:\n",
    "\n",
    "Bernoulli Naive Bayes: Typically more memory-efficient than multinomial Naive Bayes when dealing with high-dimensional sparse feature vectors, such as those encountered in text classification.\n",
    "Multinomial Naive Bayes: Can handle a larger variety of features and is more suitable for tasks where feature counts or frequencies are essential for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "Ans- In Bernoulli Naive Bayes, missing values are typically handled by treating them as an additional category or class of the feature, representing the absence of the feature. This approach aligns with the fundamental assumption of the Bernoulli Naive Bayes classifier, which assumes binary features indicating the presence or absence of certain attributes.\n",
    "\n",
    "When dealing with missing values, each feature with a missing value is essentially treated as if the feature is not present, thus implicitly representing the absence of that feature. During the training phase, the classifier learns the probability of each feature (including the missing value category) given each class label.\n",
    "\n",
    "During prediction or classification, if a feature value is missing for a particular instance, the classifier simply ignores that feature for the calculation of class probabilities. The absence of the feature is implicitly accounted for by considering the probability of the missing value category (representing the absence of the feature) for each class.\n",
    "\n",
    "It's important to note that the treatment of missing values in Bernoulli Naive Bayes is implicit and relies on the assumption that the absence of a feature is informative in itself. This approach may not always be suitable, especially if missing values are not missing at random or if the absence of a feature is not informative for classification. In such cases, other imputation techniques or handling strategies may be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "Ans- Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. \n",
    "\n",
    "In Gaussian Naive Bayes, it's assumed that the continuous features of each class are normally (Gaussian) distributed. This assumption holds for each class separately. When it comes to multi-class classification, Gaussian Naive Bayes extends this assumption to all classes involved.\n",
    "\n",
    "During the training phase, Gaussian Naive Bayes estimates the mean and variance of each feature for each class. Then, when making predictions for a new instance, it calculates the likelihood of observing the features given each class using the estimated mean and variance parameters.\n",
    "\n",
    "The class with the highest posterior probability, which is calculated using Bayes' theorem, is then assigned as the predicted class for the new instance.\n",
    "\n",
    "In summary, Gaussian Naive Bayes can handle multi-class classification by extending its Gaussian distribution assumption to each class and calculating class probabilities using Bayes' theorem."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
